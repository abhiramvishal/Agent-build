# {{ project_name }}

A small FastAPI service generated by the **Project Factory Agent**.

## Goal

> {{ goal }}

This service exposes:

- `GET /health` – readiness/liveness probe.
- `POST /generate` – generates text using local Ollama (`{{ ollama_model }}`) from a JSON body.

---

## Requirements

- Python {{ python_version }}+
- [FastAPI](https://fastapi.tiangolo.com/)
- [Uvicorn](https://www.uvicorn.org/)
- [Ollama](https://ollama.com/) running locally.
- Git (optional but recommended)

Make sure Ollama is running and the model is pulled:

```bash
ollama serve
ollama pull {{ ollama_model }}
```

---

## Setup

Create and activate a virtual environment:

```bash
python -m venv .venv
# Windows:
.venv\Scripts\activate
# macOS / Linux:
source .venv/bin/activate
```

Install the project in editable mode:

```bash
pip install -e .
```

Run tests to confirm everything passes:

```bash
pytest -q
```

---

## Running the Service

Start the FastAPI application with Uvicorn:

```bash
uvicorn app.main:app --reload
```

By default this serves on `http://127.0.0.1:8000`.

### Health Check

```bash
curl -i http://127.0.0.1:8000/health
```

You should see `HTTP/1.1 200 OK`.

### Text Generation

```bash
curl -i -X POST "http://127.0.0.1:8000/generate" \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Say hello", "max_tokens": 200}'
```

The endpoint will talk to local Ollama (`{{ ollama_model }}`) and return generated text.

---

## Development Notes

- The project is intentionally small and focused for rapid iteration.
- All AI-specific logic is routed through `app.main` in the `/generate` endpoint.
- You can customize behavior there, or add new routers/endpoints under `app/routers/`.
- To keep your repo safe, `.gitignore` covers venvs, caches, logs, data, and model dumps.

